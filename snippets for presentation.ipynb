{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = './ted_mini/art_positive/5.ted'\n",
    "delim = \" \"\n",
    "with open(file_name, 'r') as f: \n",
    "    dat = f.readlines()\n",
    "    dat = map(lambda x: x.replace('_en', '').replace(\"\\n\", delim).lower().split(delim), dat)\n",
    "\n",
    "# df = pd.DataFrame(dat).rename(columns={0:'word', 1:'count'}).sort('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "dat = list(chain(*dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td> 325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td> 188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td> 147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td> 133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td> 118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td> 107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>  98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>  97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>  87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>  85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>  79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>  77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>  74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>  69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>  62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>  56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>  49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>  46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>  40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>  35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>  33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>  32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'re</th>\n",
       "      <td>  29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>  29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>  28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>  27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n't</th>\n",
       "      <td>  26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>  25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>  25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>  24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditions</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opinions</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holders</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hip</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dr.</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collusion</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prince</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priest</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coming</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pressure</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commitment</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>henry</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physical</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pollution</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pollute</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platonic</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>places</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pin</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pictures</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picture</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postcard</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "word             \n",
       ",             325\n",
       ".             188\n",
       "and           147\n",
       "the           133\n",
       "to            118\n",
       "that          107\n",
       "a              98\n",
       "of             97\n",
       "i              87\n",
       "this           85\n",
       "we             79\n",
       "it             77\n",
       "you            74\n",
       "in             69\n",
       "is             62\n",
       "'s             56\n",
       "they           49\n",
       "\"              46\n",
       "--             40\n",
       "what           35\n",
       "was            33\n",
       "with           32\n",
       "'re            29\n",
       "have           29\n",
       "about          28\n",
       "cars           27\n",
       "n't            26\n",
       "all            25\n",
       "are            25\n",
       "so             24\n",
       "...           ...\n",
       "order           1\n",
       "conditions      1\n",
       "opinions        1\n",
       "holders         1\n",
       "hip             1\n",
       "help            1\n",
       "dr.             1\n",
       "co              1\n",
       "coffee          1\n",
       "collusion       1\n",
       "prince          1\n",
       "priest          1\n",
       "coming          1\n",
       "pressure        1\n",
       "commitment      1\n",
       "henry           1\n",
       "company         1\n",
       "pop             1\n",
       "physical        1\n",
       "pollution       1\n",
       "pollute         1\n",
       "pole            1\n",
       "compare         1\n",
       "platonic        1\n",
       "plane           1\n",
       "places          1\n",
       "pin             1\n",
       "pictures        1\n",
       "picture         1\n",
       "postcard        1\n",
       "\n",
       "[799 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "d = pd.DataFrame(list(chain(*dat))).rename(columns={0:'word'})\n",
    "d['count'] = 1\n",
    "df = d.groupby('word').agg('sum').sort('count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('./5.ted_int_output', 'w') as g:  \n",
    "    for i in range(4):\n",
    "        g.write(' '.join(dat[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"what i want to talk about is , as background , is the idea that cars are art .\n",
    "this is actually quite meaningful to me , because car designers tend to be a little bit low on the totem pole -- we do n't do coffee table books with just one lamp inside of it -- and cars are thought so much as a product that it 's a little bit difficult to get into the aesthetic side under the same sort of terminology that one would discuss art .\n",
    "and so cars , as art , brings it into an emotional plane -- if you accept that -- that you have to deal with on the same level you would with art with a capital a.\n",
    "now at this point you 're going to see a picture of michelangelo .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "cv = CountVectorizer()\n",
    "vec = cv.fit(dat)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    vec.vocabulary_.items())\n",
    "        .rename(columns={0:'word', 1:'count'})\n",
    "        .sort('count', ascending=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf(t, doc):\n",
    "    if type(doc) == list: \n",
    "        return float(doc.count(t))/float(len(doc))\n",
    "    elif type(doc) == str: \n",
    "        doc = text_to_bagofwords(doc)\n",
    "        return float(doc.count(t))/float(len(doc))\n",
    "    else: \n",
    "        return NaN\n",
    "\n",
    "def idf(t, corpus):\n",
    "    df = 0\n",
    "    for doc in corpus: \n",
    "        if doc.count(t) > 0: \n",
    "            df += 1\n",
    "    return float(len(corpus))/float(df + 0.01)\n",
    "\n",
    "def tfidf(t, doc, corpus):\n",
    "    return tf(t, doc)*idf(t, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vec = TfidfVectorizer()\n",
    "# tfidf = vec.fit_transform(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x10640c170>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lda\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)\n",
    "ntop=10\n",
    "for i, topic_dist in enumerate(model.topic_word_[:ntop]):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(ntop+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 UK: Prince Charles spearheads British royal revolution. LONDON 1996-08-20 (top topic: 8)\n",
      "1 GERMANY: Historic Dresden church rising from WW2 ashes. DRESDEN, Germany 1996-08-21 (top topic: 13)\n",
      "2 INDIA: Mother Teresa's condition said still unstable. CALCUTTA 1996-08-23 (top topic: 14)\n",
      "3 UK: Palace warns British weekly over Charles pictures. LONDON 1996-08-25 (top topic: 8)\n",
      "4 INDIA: Mother Teresa, slightly stronger, blesses nuns. CALCUTTA 1996-08-25 (top topic: 14)\n",
      "5 INDIA: Mother Teresa's condition unchanged, thousands pray. CALCUTTA 1996-08-25 (top topic: 14)\n",
      "6 INDIA: Mother Teresa shows signs of strength, blesses nuns. CALCUTTA 1996-08-26 (top topic: 14)\n",
      "7 INDIA: Mother Teresa's condition improves, many pray. CALCUTTA, India 1996-08-25 (top topic: 14)\n",
      "8 INDIA: Mother Teresa improves, nuns pray for \"miracle\". CALCUTTA 1996-08-26 (top topic: 14)\n",
      "9 UK: Charles under fire over prospect of Queen Camilla. LONDON 1996-08-26 (top topic: 8)\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "stoplist = stopwords.words('english')\n",
    "texts = map(lambda y: \n",
    "        filter(lambda x: len(x) > 2, y), \n",
    "        texts)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in STOPWORDS\n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids) \n",
    "dictionary.compactify()\n",
    "dictionary.save(os.path.join(outputdir, 'artsci_positive_corpus.dict'))\n",
    "\n",
    "artsci_corpus = [dictionary.doc2bow(text) for text in texts if text not in stoplist]\n",
    "corpora.MmCorpus.serialize(os.path.join(outputdir, 'artsci_positive_corpus.mm'), artsci_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel(corpus=artsci_corpus, id2word=dictionary, num_topics=20, update_every=1, chunksize=100, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.011*know + 0.009*one + 0.009*going + 0.008*like + 0.008*said + 0.007*think + 0.007*really + 0.006*time + 0.006*see + 0.006*people',\n",
       " '0.009*one + 0.008*like + 0.007*people + 0.007*going + 0.007*things + 0.007*know + 0.007*well + 0.006*get + 0.006*really + 0.006*time',\n",
       " '0.011*one + 0.010*know + 0.009*going + 0.007*think + 0.007*like + 0.007*see + 0.006*things + 0.006*get + 0.006*something + 0.005*well',\n",
       " '0.010*like + 0.009*one + 0.008*get + 0.008*said + 0.007*see + 0.007*going + 0.006*people + 0.006*know + 0.005*would + 0.005*really',\n",
       " '0.009*like + 0.009*one + 0.008*know + 0.007*world + 0.006*think + 0.006*people + 0.006*things + 0.006*time + 0.005*would + 0.005*going']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-295-a11df7870d81>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-295-a11df7870d81>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    words = [“I”, “like”, “enjoy”, “language”, “learning”, “natural”, “processing”, “.”]\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "la = np.linalg\n",
    "words = ['I like data science and data discovery.',\n",
    "         'I think data science requires data exploration and machine learning', \n",
    "         'I apply machine learning to data for science discovery']\n",
    "\n",
    "X = np.array([[0, 2, 1, 0, 0, 0, 1, 0], [1, 0, 1, 0, 0, 0, 1, 0], [1, 2, 0, 0, 0, 0, 1, 0], [1, 2, 1, 0, 0, 0, 1, 0], [1, 2, 1, 0, 0, 0, 1, 0], [1, 2, 1, 0, 0, 0, 1, 0], [1, 2, 1, 0, 0, 0, 0, 1], [1, 2, 1, 0, 0, 0, 1, 0]])\n",
    "U, s, Vh = la.svd(X, full_matrices=False)\n",
    "for i in xrange(len(words)):\n",
    "plt.text(U[i,0], U[i, 1], words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'computer',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'did',\n",
       "           'do',\n",
       "           'does',\n",
       "           'doesn',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fify',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'just',\n",
       "           'keep',\n",
       "           'kg',\n",
       "           'km',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'quite',\n",
       "           'rather',\n",
       "           're',\n",
       "           'really',\n",
       "           'regarding',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'unless',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'used',\n",
       "           'using',\n",
       "           'various',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7456"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(once_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
