{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "ANSWER: Text Processing Exercise: Turning text into bag or words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1.1 Read text data (raw TED transcripts) from '5.ted' from './ted_mini/art_positive/' directory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hint: You can use readline() or readlines() to read text file\n",
      "# How many lines are in 5.ted transcripts?\n",
      "dPath = './ted_mini/art_positive/'\n",
      "with open('%s/%s' % (dPath, '5.ted'), 'r') as f: \n",
      "    dat = f.readlines()\n",
      "    \n",
      "print \"Number of line : %s\" % str(len(dat))\n",
      "\n",
      "# Do you notice anything 'unusal' about the transcripts?\n",
      "## There are _en after every word and \\n after every line. Let's remove them\n",
      "dat = map(lambda x: x.replace(\"_en\", \"\").strip(\"\\n\").lower().split(' '), dat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of line : 166\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1.2: Tokenize text into words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How many tokens are in this transcript, how many 'unique' ones?\n",
      "from itertools import chain\n",
      "flat_dat = list(chain(*dat))\n",
      "print \"Number of total tokens : %s\" % str(len(flat_dat))\n",
      "print \"Number of unique tokens : %s\" % str(len(set(flat_dat)))\n",
      "\n",
      "## Do you notice additional things that should be removed?\n",
      "## one character tokens, tokens that are only digits, tokens should be removed\n",
      "## tokens should also have punctuations removed\n",
      "# print flat_dat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of total tokens : 4534\n",
        "Number of unique tokens : 799\n"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1.3: Remove Punctuations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example: \n",
      "import re, string\n",
      "pattern = '[{}]'.format(re.escape(string.punctuation))\n",
      "my_str = \",Welcome! to the world of 'alphanumeric!!!- We remove punctuations!'\"\n",
      "print re.sub('[{}]'.format(re.escape(string.punctuation)), '', my_str.lower())\n",
      "print ' '.join(re.findall(r'\\w+', my_str.lower()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "welcome to the world of alphanumeric we remove punctuations\n",
        "welcome to the world of alphanumeric we remove punctuations\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Hint 1: You can use string.punctuation to list all ASCII punctuation characters. (https://docs.python.org/2/library/string.html) \n",
      "# Use list comprehension\n",
      "flat_dat_nopunc = [re.sub('[{}]'.format(re.escape(string.punctuation)), '', x) for x in flat_dat]\n",
      "# Use MAP\n",
      "flat_dat_nopunc = map(lambda x: re.sub('[{}]'.format(re.escape(string.punctuation)), '', x), flat_dat)\n",
      "\n",
      "## Hint 2: You can use regex pattern r'\\w+' to match any alphanumeric and underscore (equivalent to the set [a-zA-Z0-9_]).\n",
      "# Use list comprehension\n",
      "flat_dat_nopunc = [''.join(re.findall(r'\\w+', x)) for x in flat_dat]\n",
      "# Use MAP\n",
      "flat_dat_nopunc = map(lambda x: ''.join(re.findall(r'\\w+', x)), flat_dat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1.4: Any Additional cleanup?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## There are empty tokens and tokens that only contain digits. Let's remove them\n",
      "# Use list comprehension\n",
      "flat_dat_nodigit = [x for x in flat_dat_nopunc if (len(x) > 1) & (not x.isdigit())]\n",
      "# Use FILTER\n",
      "flat_dat_nodigit = filter(lambda x: len(x) > 1 and not x.isdigit() , flat_dat_nopunc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1.5 Lemmatization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can use the following libraries to lemmatize text\n",
      "# 1. WordNet Lemmatizer: \n",
      "# from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "lmtzr = WordNetLemmatizer()\n",
      "flat_dat_lemmatized = [lmtzr.lemmatize(x) for x in flat_dat_nodigit]\n",
      "flat_dat_lemmatized = map(lambda x: lmtzr.lemmatize(x), flat_dat_nodigit)\n",
      "\n",
      "# 2. Porter Stemmer: \n",
      "# from nltk.stem import PorterStemmer\n",
      "from nltk.stem import PorterStemmer\n",
      "pts = PorterStemmer()\n",
      "flat_dat_stemmed = [pts.stem(x) for x in flat_dat_nodigit]\n",
      "flat_dat_stemmed = map(lambda x: pts.stem(x), flat_dat_nodigit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise 1.6: Bonus Remove stop words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can use stopwords in nltk english corpus\n",
      "# nltk.corpus.stopwords.words('english')\n",
      "import nltk\n",
      "stop_words = nltk.corpus.stopwords.words('english')\n",
      "flat_dat_nostop = filter(lambda x: x not in stop_words, flat_dat_lemmatized)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}