{
 "metadata": {
  "name": "",
  "signature": "sha256:94df228d901d7702c3b6432152cfcb5e19596e7d416c5409929a5003493ead2c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cleaning data from scratch"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "art_dir = os.path.join(os.getcwd(), 'ted_mini', 'art_positive')\n",
      "sci_dir = art_dir.replace('art', 'sci')\n",
      "art_doc = filter(lambda a: a.endswith('.ted'), os.listdir(art_dir))\n",
      "sci_doc = filter(lambda a: a.endswith('.ted'), os.listdir(sci_dir))\n",
      "\n",
      "from itertools import chain\n",
      "import re\n",
      "\n",
      "def raw2doc(raw):\n",
      "    text = map(lambda x: x.replace('_en', '').decode('utf-8', 'ignore'), raw)\n",
      "    text = map(lambda x: ' '.join(re.findall(r'\\w+', x)), text)\n",
      "    return ' '.join(text).split(' ')\n",
      "\n",
      "texts = []\n",
      "for a in art_doc: \n",
      "    with open(os.path.join(art_dir, a), 'r') as f: \n",
      "        text = raw2doc(f.readlines())\n",
      "        texts.append(text)\n",
      "        with open(os.path.join(outputdir, a + '.art'), 'w') as g: \n",
      "            g.write(' '.join(text))\n",
      "            \n",
      "for s in sci_doc: \n",
      "    with open(os.path.join(sci_dir, s), 'r') as f: \n",
      "        text = raw2doc(f.readlines())\n",
      "        texts.append(text)\n",
      "        with open(os.path.join(outputdir, s + '.sci'), 'w') as g: \n",
      "            g.write(' '.join(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building Corpus and Dictionary from raw data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "from gensim import corpora, models\n",
      "from gensim.parsing.preprocessing import STOPWORDS\n",
      "\n",
      "data_dir = os.path.join(os.getcwd(), 'ted_mini', 'artandsci_positive')\n",
      "docs = filter(lambda x: '.ted.' in x, os.listdir(data_dir))\n",
      "texts = []\n",
      "for doc in docs:\n",
      "    with open(os.path.join(data_dir, doc), 'r') as f:\n",
      "        texts.append(f.readlines()[0].split(' '))\n",
      "\n",
      "texts = map(lambda x: filter(lambda y: len(y) > 2, x), texts)\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
      "stop_ids = [dictionary.token2id[stopword] for stopword in STOPWORDS\n",
      "            if stopword in dictionary.token2id]\n",
      "dictionary.filter_tokens(stop_ids + once_ids) \n",
      "dictionary.compactify()\n",
      "# Uncomment if you want to save the dictionary\n",
      "# dictionary.save(os.path.join(data_dir, 'artsci_positive_corpus.dict'))\n",
      "\n",
      "artsci_corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "# Uncomment if you want to save the corpus\n",
      "# corpora.MmCorpus.serialize(os.path.join(data_dir, 'artsci_positive_corpus.mm'), artsci_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Topic modeling using gensim LDA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.ldamodel import LdaModel\n",
      "lda = LdaModel(corpus=artsci_corpus, id2word=dictionary, \n",
      "               num_topics=20, update_every=1, chunksize=100, passes=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Example topic\n",
      "lda.show_topic(11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "[(0.015640414022064685, 'earth'),\n",
        " (0.012160429080861469, 'stars'),\n",
        " (0.011924733515645118, 'like'),\n",
        " (0.011199539194868741, 'universe'),\n",
        " (0.010963916617258741, 'atoms'),\n",
        " (0.0095423565216509812, 'century'),\n",
        " (0.0085391585978015876, 'einstein'),\n",
        " (0.0079199531795733497, 'billion'),\n",
        " (0.0074580017159042661, 'planets'),\n",
        " (0.0073421488411868473, 'small')]"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Example topic\n",
      "lda.show_topic(15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "[(0.019069935795224396, 'ants'),\n",
        " (0.012175520975191553, 'nest'),\n",
        " (0.009606874068946494, 'ant'),\n",
        " (0.0095467406982767469, 'people'),\n",
        " (0.0092058530459285474, 'colony'),\n",
        " (0.0072401546937930416, 'like'),\n",
        " (0.0069742669948207143, 'workers'),\n",
        " (0.0062076249103784975, 'years'),\n",
        " (0.0060125350934175431, 'city'),\n",
        " (0.0054315333587530582, 'world')]"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Word2Vec"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.word2vec import Word2Vec\n",
      "w2vmodel = Word2Vec(texts, size=100, window=5, min_count=5, workers=2)\n",
      "w2vmodel.save(os.join.path(data_dir), 'artsci_positive_w2vmodel')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w2vmodel.similarity('man', 'woman')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "0.71450582833706511"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar(positive=['cambridge', 'brain'], negative=['pittsburgh'], topn=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "[('protein', 0.6212430000305176)]"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.doesnt_match(\"breakfast food neurons dinner\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 115,
       "text": [
        "'neurons'"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}